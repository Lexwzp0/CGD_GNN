# python3 /home/wangzepeng/Augmentation/train_utils/train_eval_classifier.py
import sys
import os
import argparse
from torch.utils.data import random_split

# è·å–å½“å‰æ–‡ä»¶çš„ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
# å‡è®¾ src ç›®å½•åœ¨å½“å‰ç›®å½•çš„ä¸Šä¸€çº§
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

# ç°åœ¨å°è¯•å¯¼å…¥ - ç§»é™¤æ‰©æ•£æ¨¡å‹å¯¼å…¥
from src.models.classifier import UNetClassifier
from src.utils.pyg_dataToGraph import DataToGraph
import torch.nn.functional as F
from matplotlib import pyplot as plt
import torch
from torch.utils.data import DataLoader, TensorDataset
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm

def main(args):
    # -- 1. è®¾ç½®å’Œé…ç½® --
    os.makedirs(args.model_dir, exist_ok=True)
    os.makedirs(args.plot_dir, exist_ok=True)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # -- 2. æ•°æ®åŠ è½½å’Œå‡†å¤‡ --
    dataset = DataToGraph(
        raw_data_path=args.data_path,
        dataset_name='TFF' + '.mat'
    )
    num_classes = dataset.num_classes

    x0, labels = [], []
    for data in dataset:
        x0.append(data.x)
        labels.append(data.y)

    x0 = torch.stack(x0).unsqueeze(1)
    labels = torch.stack(labels)
    
    print(f"åŸå§‹æ•°æ®é›†å¤§å°: {len(dataset)}")
    print(f"ç±»åˆ«æ•°: {num_classes}")
    print("X0 shape:", x0.shape)
    print("Labels shape:", labels.shape)

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    full_dataset = TensorDataset(x0, labels)
    val_size = int(len(full_dataset) * args.val_split)
    train_size = len(full_dataset) - val_size
    
    torch.manual_seed(42)
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=4)

    # -- 3. æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨åˆå§‹åŒ– --
    eval_classifier = UNetClassifier(num_classes).to(device)
    optimizer = AdamW(eval_classifier.parameters(), lr=args.lr)
    scheduler = CosineAnnealingLR(optimizer, T_max=args.num_epochs)

    # -- 4. è®­ç»ƒå¾ªç¯ --
    history = {
        'train_loss': [], 'train_acc': [],
        'val_loss': [], 'val_acc': []
    }
    best_val_acc = 0.0
    
    print("ğŸ¯ å¼€å§‹è®­ç»ƒè¯„ä¼°åˆ†ç±»å™¨ï¼ˆåœ¨å¹²å‡€æ•°æ®ä¸Šï¼‰...")

    for epoch in range(args.num_epochs):
        # --- è®­ç»ƒé˜¶æ®µ ---
        eval_classifier.train()
        train_loss, train_correct, train_total = 0, 0, 0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{args.num_epochs} [è®­ç»ƒ]", unit="batch")
        
        for x_batch, label_batch in pbar:
            x_batch, label_batch = x_batch.to(device), label_batch.to(device)
            if label_batch.dim() > 1:
                label_batch = label_batch.squeeze(1)

            # å…³é”®ï¼šåœ¨å¹²å‡€æ•°æ®ä¸Šè®­ç»ƒï¼Œä¼ å…¥è™šæ‹Ÿçš„æ—¶é—´æ­¥ t=0
            dummy_timesteps = torch.zeros(x_batch.size(0), device=device).long()
            logits = eval_classifier(x_batch, dummy_timesteps)
            
            loss = F.cross_entropy(logits, label_batch)

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(eval_classifier.parameters(), args.grad_clip)
            optimizer.step()

            train_loss += loss.item() * x_batch.size(0)
            _, predicted = torch.max(logits, 1)
            train_correct += (predicted == label_batch).sum().item()
            train_total += label_batch.size(0)
            
            pbar.set_postfix({
                'Loss': f"{loss.item():.4f}",
                'Acc': f"{train_correct / train_total:.2%}"
            })

        avg_train_loss = train_loss / train_total
        avg_train_acc = train_correct / train_total
        history['train_loss'].append(avg_train_loss)
        history['train_acc'].append(avg_train_acc)

        # --- éªŒè¯é˜¶æ®µ ---
        eval_classifier.eval()
        val_loss, val_correct, val_total = 0, 0, 0
        with torch.no_grad():
            for x_batch, label_batch in val_loader:
                x_batch, label_batch = x_batch.to(device), label_batch.to(device)
                if label_batch.dim() > 1:
                    label_batch = label_batch.squeeze(1)

                dummy_timesteps = torch.zeros(x_batch.size(0), device=device).long()
                logits = eval_classifier(x_batch, dummy_timesteps)
                
                loss = F.cross_entropy(logits, label_batch)
                
                val_loss += loss.item() * x_batch.size(0)
                _, predicted = torch.max(logits, 1)
                val_correct += (predicted == label_batch).sum().item()
                val_total += label_batch.size(0)
        
        avg_val_loss = val_loss / val_total
        avg_val_acc = val_correct / val_total
        history['val_loss'].append(avg_val_loss)
        history['val_acc'].append(avg_val_acc)
        
        scheduler.step()

        print(f"Epoch {epoch + 1:03d} | Train Loss: {avg_train_loss:.4f}, Acc: {avg_train_acc:.2%} | "
              f"Val Loss: {avg_val_loss:.4f}, Acc: {avg_val_acc:.2%} | "
              f"LR: {scheduler.get_last_lr()[0]:.2e}")

        # æ ¹æ®éªŒè¯é›†å‡†ç¡®ç‡ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_acc > best_val_acc:
            best_val_acc = avg_val_acc
            save_path = os.path.join(args.model_dir, 'best_eval_classifier.pth')
            torch.save(eval_classifier.state_dict(), save_path)
            print(f"âœ… ä¿å­˜æœ€ä½³è¯„ä¼°åˆ†ç±»å™¨åˆ° {save_path} | éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2%}")

    # -- 5. ç»“æœå¯è§†åŒ–å’Œä¿å­˜ --
    plt.figure(figsize=(14, 6))
    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Training Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.title("è¯„ä¼°åˆ†ç±»å™¨æŸå¤±æ›²çº¿")
    plt.xlabel("Epoch")
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 2, 2)
    plt.plot(history['train_acc'], label='Training Accuracy')
    plt.plot(history['val_acc'], label='Validation Accuracy')
    plt.title("è¯„ä¼°åˆ†ç±»å™¨å‡†ç¡®ç‡æ›²çº¿")
    plt.xlabel("Epoch")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plot_path = os.path.join(args.plot_dir, 'eval_classifier_training_curves.png')
    plt.savefig(plot_path)
    plt.show()
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆ! è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜åˆ° {plot_path}")
    print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2%}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="ä¸ºå¼•å¯¼æ‰©æ•£æ¨¡å‹è®­ç»ƒä¸€ä¸ªåœ¨å¹²å‡€æ•°æ®ä¸Šçš„è¯„ä¼°åˆ†ç±»å™¨")
    
    # è·¯å¾„å’Œç›®å½•å‚æ•°
    parser.add_argument('--data_path', type=str, default='/data/wangzepeng/raw', help='åŸå§‹æ•°æ®çš„è·¯å¾„')
    parser.add_argument('--model_dir', type=str, default='/data/wangzepeng/models/classifier', help='æ¨¡å‹æ£€æŸ¥ç‚¹çš„ä¿å­˜ç›®å½•')
    parser.add_argument('--plot_dir', type=str, default='/home/wangzepeng/Augmentation/results/classifier', help='è®­ç»ƒæ›²çº¿å›¾çš„ä¿å­˜ç›®å½•')

    # è®­ç»ƒè¶…å‚æ•°
    parser.add_argument('--num_epochs', type=int, default=1000, help='è®­ç»ƒçš„æ€»è½®æ•°')
    parser.add_argument('--batch_size', type=int, default=64, help='è®­ç»ƒçš„æ‰¹é‡å¤§å°')
    parser.add_argument('--lr', type=float, default=3e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--grad_clip', type=float, default=1.0, help='æ¢¯åº¦è£å‰ªçš„é˜ˆå€¼')
    parser.add_argument('--val_split', type=float, default=0.2, help='éªŒè¯é›†æ‰€å çš„æ¯”ä¾‹')
    
    args = parser.parse_args()
    main(args)
