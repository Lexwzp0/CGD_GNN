# python3 /home/wangzepeng/CGD_GNN/train_utils/train_Unet_sample.py
import sys
import os
import argparse
from torch.utils.data import random_split

# è·å–å½“å‰æ–‡ä»¶çš„ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
# å‡è®¾ src ç›®å½•åœ¨å½“å‰ç›®å½•çš„ä¸Šä¸€çº§
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

import torch
# from data.pyg_dataToGraph import DataToGraph
from src.utils.pyg_dataToGraph import DataToGraph # æ›´æ–°è·¯å¾„
import torch.nn.functional as F
from matplotlib import pyplot as plt
# from src.Unet import ConditionalDiffusionUNet
from src.models.Unet import ConditionalDiffusionUNet # æ›´æ–°è·¯å¾„
from src.models.classifier import UNetClassifier # é‡æ–°å¼•å…¥
import torch
from torch.utils.data import DataLoader, TensorDataset
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm
# from src.gaussian_diffusion import GuidedGaussianDiffusion
from src.models.gaussian_diffusion import GuidedGaussianDiffusion # æ›´æ–°è·¯å¾„
import seaborn as sns # æ–°å¢ï¼Œç”¨äºç»˜å›¾
import numpy as np # æ–°å¢ï¼Œç”¨äºæ•°å­¦è¿ç®—

def main(args):
    # -- 1. è®¾ç½®å’Œé…ç½® --
    os.makedirs(args.model_dir, exist_ok=True)
    os.makedirs(args.plot_dir, exist_ok=True)
    # ç§»é™¤betaè°ƒåº¦ç›¸å…³çš„ç›®å½•åŒºåˆ†
    heatmap_dir = os.path.join(args.plot_dir, 'epoch_heatmaps')
    os.makedirs(heatmap_dir, exist_ok=True)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # -- 2. æ•°æ®åŠ è½½å’Œå‡†å¤‡ --
    dataset = DataToGraph(
        raw_data_path=args.data_path,
        dataset_name='TFF' + '.mat'
    )
    num_classes = dataset.num_classes

    x0, labels = [], []
    for data in dataset:
        x0.append(data.x)
        labels.append(data.y)

    x0 = torch.stack(x0).unsqueeze(1)
    labels = torch.stack(labels)

    print(f"åŸå§‹æ•°æ®é›†å¤§å°: {len(dataset)}")
    print(f"ç±»åˆ«æ•°: {num_classes}")
    print("X0 shape:", x0.shape)
    print("Labels shape:", labels.shape)

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    full_dataset = TensorDataset(x0, labels)
    val_size = int(len(full_dataset) * args.val_split)
    train_size = len(full_dataset) - val_size
    
    torch.manual_seed(42)
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=4)
    
    # -- 3. æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨åˆå§‹åŒ– --
    model = ConditionalDiffusionUNet(
        num_classes=num_classes,
        time_dim=128,
        label_dim=64
    ).to(device)
    
    # é‡æ–°åŠ è½½è¯„ä¼°åˆ†ç±»å™¨ä½œä¸ºå ä½ç¬¦
    print(f"åŠ è½½è¯„ä¼°åˆ†ç±»å™¨: {args.eval_classifier_path}")
    eval_classifier = UNetClassifier(num_classes).to(device)
    try:
        eval_classifier.load_state_dict(torch.load(args.eval_classifier_path, map_location=device))
        eval_classifier.eval()
    except Exception as e:
        print(f"åŠ è½½è¯„ä¼°åˆ†ç±»å™¨å¤±è´¥: {e}"); sys.exit(1)
        
    diffusion = GuidedGaussianDiffusion(
        num_steps=args.diffusion_steps
    )
    optimizer = AdamW(model.parameters(), lr=args.lr)
    scheduler = CosineAnnealingLR(optimizer, T_max=args.num_epochs)

    # -- 4. è®­ç»ƒå¾ªç¯ --
    history = {'train_loss': [], 'val_loss': []}
    best_val_loss = float('inf')

    for epoch in range(args.num_epochs):
        # --- è®­ç»ƒé˜¶æ®µ ---
        model.train()
        train_loss = 0.0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{args.num_epochs} [è®­ç»ƒ]", unit="batch")
        
        for x_batch, label_batch in pbar:
            x_batch, label_batch = x_batch.to(device), label_batch.to(device)
            
            t = torch.randint(0, diffusion.num_steps, (x_batch.size(0),), device=device).long()
            
            losses = diffusion.training_losses(model=model, x_start=x_batch, t=t, batch_labels=label_batch)
            loss = losses["loss"].mean()

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)
            optimizer.step()

            train_loss += loss.item()
            pbar.set_postfix({
                'Loss': loss.item(),
                'MSE': losses['mse'].mean().item(),
                'VB': losses['vb'].mean().item()
            })

        avg_train_loss = train_loss / len(train_loader)
        history['train_loss'].append(avg_train_loss)

        # --- éªŒè¯é˜¶æ®µ ---
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for x_batch, label_batch in val_loader:
                x_batch, label_batch = x_batch.to(device), label_batch.to(device)
                t = torch.randint(0, diffusion.num_steps, (x_batch.size(0),), device=device).long()
                losses = diffusion.training_losses(model=model, x_start=x_batch, t=t, batch_labels=label_batch)
                loss = losses["loss"].mean()
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        history['val_loss'].append(avg_val_loss)

        scheduler.step()

        print(f"Epoch {epoch + 1:03d} | Train Loss: {avg_train_loss:.4e} | Val Loss: {avg_val_loss:.4e} | LR: {scheduler.get_last_lr()[0]:.2e}")

        # --- Intermittent Sampling and Visualization ---
        if (epoch + 1) % args.sample_interval == 0 or (epoch + 1) == args.num_epochs:
            run_intermittent_eval(model, diffusion, eval_classifier, device, num_classes, args, epoch, heatmap_dir)
            
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            # ç§»é™¤æ–‡ä»¶åä¸­çš„betaè°ƒåº¦ä¿¡æ¯
            model_filename = 'best_unet.pth'
            save_path = os.path.join(args.model_dir, model_filename)
            torch.save(model.state_dict(), save_path)
            print(f"âœ… ä¿å­˜æœ€ä½³U-Netæ¨¡å‹åˆ° {save_path} | éªŒè¯æŸå¤±: {best_val_loss:.4e}")

    # -- 5. ç»“æœå¯è§†åŒ–å’Œä¿å­˜ --
    plt.figure(figsize=(12, 6))
    plt.plot(history['train_loss'], label='Training Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.title("U-Net Training and Validation Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ç§»é™¤æ–‡ä»¶åä¸­çš„betaè°ƒåº¦ä¿¡æ¯
    plot_filename = 'unet_loss_curve.png'
    plot_path = os.path.join(args.plot_dir, plot_filename)
    plt.savefig(plot_path)
    plt.show()

    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ˆ è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜åˆ° {plot_path}")
    print(f"ğŸ† æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4e}")

def run_intermittent_eval(unet_model, diffusion, eval_classifier, device, num_classes, args, epoch, heatmap_dir):
    print(f"\n--- Running intermittent evaluation at epoch {epoch + 1} ---")
    unet_model.eval()
    
    # ä½¿ç”¨æ— å¼•å¯¼çš„é‡‡æ ·æ¥è¯„ä¼°U-Netæœ¬èº«çš„èƒ½åŠ›
    original_scale = diffusion.classifier_scale
    diffusion.classifier_scale = 0.0
    
    # è®¾ç½®å­å›¾å¸ƒå±€
    num_cols = min(num_classes, 4) # æ¯è¡Œæœ€å¤š4ä¸ª
    num_rows = (num_classes + num_cols - 1) // num_cols
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 5, num_rows * 4))
    axes = axes.flatten() # å°†2Dæ•°ç»„å±•å¹³ä¸º1Dï¼Œæ–¹ä¾¿ç´¢å¼•

    try:
        with torch.no_grad():
            for class_idx in range(num_classes):
                # ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆä¸€ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
                target_labels = torch.full((1,), class_idx, device=device).long()
                
                generated = diffusion.p_sample_loop(
                    model=unet_model,
                    classifier=eval_classifier, # ä¼ å…¥åˆ†ç±»å™¨ä½œä¸ºå ä½ç¬¦
                    shape=(1, 1, 24, 50),
                    batch_labels=target_labels,
                    progress=False
                )
                
                # è·å–æ ·æœ¬å¼ é‡å¹¶ç§»è‡³CPU
                sample_tensor = generated[0, 0].cpu().numpy()
                ax = axes[class_idx]

                # åœ¨å¯¹åº”çš„å­å›¾ä¸Šç»˜åˆ¶çƒ­åŠ›å›¾
                sns.heatmap(sample_tensor, cmap="viridis", ax=ax)
                ax.set_title(f'Class {class_idx}')
                ax.set_xlabel("Features")
                ax.set_ylabel("Nodes")
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(num_classes, len(axes)):
            axes[i].set_visible(False)
            
        fig.suptitle(f'Epoch {epoch + 1} - Generated Samples', fontsize=16)
        fig.tight_layout(rect=[0, 0, 1, 0.96]) # è°ƒæ•´å¸ƒå±€ä»¥é€‚åº”ä¸»æ ‡é¢˜
        
        # ä¿å­˜æ•´ä¸ªå›¾åƒ
        heatmap_path = os.path.join(heatmap_dir, f"samples_epoch_{epoch+1}.png")
        plt.savefig(heatmap_path)
        plt.close(fig)

    finally:
        # æ¢å¤åŸå§‹çš„åˆ†ç±»å™¨å¼•å¯¼å¼ºåº¦
        diffusion.classifier_scale = original_scale

    print(f"Saved intermittent sample heatmaps for epoch {epoch + 1} to: {heatmap_dir}\n")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="è®­ç»ƒç”¨äºæ¡ä»¶æ‰©æ•£çš„U-Netæ¨¡å‹ï¼Œå¹¶å®šæœŸè¿›è¡Œé‡‡æ ·è¯„ä¼°")
    
    # è·¯å¾„å’Œç›®å½•å‚æ•°
    parser.add_argument('--data_path', type=str, default='/data/wangzepeng/raw', help='åŸå§‹æ•°æ®çš„è·¯å¾„')
    parser.add_argument('--model_dir', type=str, default='/data/wangzepeng/models/unet_sample', help='U-Netæ¨¡å‹æ£€æŸ¥ç‚¹çš„ä¿å­˜ç›®å½•')
    parser.add_argument('--plot_dir', type=str, default='/home/wangzepeng/CGD_GNN/results/unet_sample', help='è®­ç»ƒæ›²çº¿å›¾çš„ä¿å­˜ç›®å½•')
    parser.add_argument('--eval_classifier_path', type=str, default='/data/wangzepeng/models/classifier/best_eval_classifier.pth',
                        help='ç”¨äºè¯„ä¼°/å ä½çš„(clean)åˆ†ç±»å™¨æ¨¡å‹è·¯å¾„')

    # è®­ç»ƒè¶…å‚æ•°
    parser.add_argument('--num_epochs', type=int, default=1000, help='è®­ç»ƒçš„æ€»è½®æ•°')
    parser.add_argument('--batch_size', type=int, default=64, help='è®­ç»ƒçš„æ‰¹é‡å¤§å°')
    parser.add_argument('--lr', type=float, default=3e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--grad_clip', type=float, default=1.0, help='æ¢¯åº¦è£å‰ªçš„é˜ˆå€¼')
    parser.add_argument('--val_split', type=float, default=0.2, help='éªŒè¯é›†æ‰€å çš„æ¯”ä¾‹')
    
    # é‡‡æ ·è¯„ä¼°å‚æ•°
    parser.add_argument('--sample_interval', type=int, default=10, help='æ¯éš”å¤šå°‘ä¸ªepochè¿›è¡Œä¸€æ¬¡é‡‡æ ·è¯„ä¼°')
    
    # æ¨¡å‹ç›¸å…³å‚æ•°
    parser.add_argument('--diffusion_steps', type=int, default=64, help='æ‰©æ•£è¿‡ç¨‹çš„æ­¥æ•°')

    args = parser.parse_args()
    main(args)